#!/usr/bin/env python3
"""
–§–ê–ó–ê 1: –ó–∞–≥—Ä—É–∑–∫–∞ HTML –∏–∑ Firecrawl –≤ staging —Ç–∞–±–ª–∏—Ü—É
================================================================
–¶–µ–ª—å: –°–æ—Ö—Ä–∞–Ω–∏—Ç—å HTML –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü Autumna –≤ staging —Ç–∞–±–ª–∏—Ü—É –æ–¥–∏–Ω —Ä–∞–∑

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python phase1_load_html.py --urls urls.txt --api-key FIRECRAWL_API_KEY

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
    - –°–ø–∏—Å–æ–∫ URL –≤ —Ñ–∞–π–ª–µ (–ø–æ –æ–¥–Ω–æ–º—É –Ω–∞ —Å—Ç—Ä–æ–∫—É)
    - Firecrawl API –∫–ª—é—á
    - PostgreSQL –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ –≤ .env
"""

import os
import sys
import json
import time
import argparse
import psycopg2
from psycopg2.extras import execute_values
from dotenv import load_dotenv
import requests
from typing import List, Dict, Optional
import re

load_dotenv()

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
FIRECRAWL_API_URL = "https://api.firecrawl.dev/v1/scrape"
BATCH_SIZE = 50  # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è Firecrawl
RETRY_DELAY = 5  # –°–µ–∫—É–Ω–¥ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏


def extract_cqc_id_from_url(url: str) -> Optional[str]:
    """–ò–∑–≤–ª–µ—á—å CQC Location ID –∏–∑ URL Autumna"""
    match = re.search(r'/1-(\d{10})', url)
    if match:
        return f"1-{match.group(1)}"
    return None


def get_db_connection():
    """–ü–æ–ª—É—á–∏—Ç—å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL"""
    return psycopg2.connect(
        host=os.getenv('DB_HOST', 'localhost'),
        port=os.getenv('DB_PORT', '5432'),
        database=os.getenv('DB_NAME', 'care_homes'),
        user=os.getenv('DB_USER', 'postgres'),
        password=os.getenv('DB_PASSWORD', '')
    )


def scrape_urls_with_firecrawl(urls: List[str], api_key: str) -> List[Dict]:
    """
    –û—Ç–ø—Ä–∞–≤–∏—Ç—å URLs –≤ Firecrawl API –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞
    
    Returns:
        List[Dict] —Å –∫–ª—é—á–∞–º–∏: url, html_content, metadata, status
    """
    results = []
    
    # Firecrawl –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç batch –∑–∞–ø—Ä–æ—Å—ã
    for i in range(0, len(urls), BATCH_SIZE):
        batch = urls[i:i+BATCH_SIZE]
        print(f"üì• –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ {i//BATCH_SIZE + 1}/{(len(urls)-1)//BATCH_SIZE + 1} ({len(batch)} URLs)...")
        
        try:
            # –û—Ç–ø—Ä–∞–≤–∏—Ç—å –∑–∞–ø—Ä–æ—Å –≤ Firecrawl
            response = requests.post(
                f"{FIRECRAWL_API_URL}/batch",
                headers={
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "urls": batch,
                    "format": "html"
                },
                timeout=300  # 5 –º–∏–Ω—É—Ç –Ω–∞ –±–∞—Ç—á
            )
            
            if response.status_code != 200:
                print(f"‚ùå –û—à–∏–±–∫–∞ Firecrawl: {response.status_code} - {response.text}")
                # –î–æ–±–∞–≤–∏—Ç—å –ø—É—Å—Ç—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –æ—à–∏–±–∫–æ–π
                for url in batch:
                    results.append({
                        'url': url,
                        'html_content': None,
                        'metadata': {'status': 'error', 'error': response.text},
                        'status': 'error'
                    })
                continue
            
            data = response.json()
            
            # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            for item in data.get('data', []):
                results.append({
                    'url': item.get('url', ''),
                    'html_content': item.get('content', ''),
                    'metadata': {
                        'status': item.get('status', 'unknown'),
                        'scraped_at': item.get('metadata', {}).get('timestamp', time.strftime('%Y-%m-%dT%H:%M:%SZ')),
                        'title': item.get('metadata', {}).get('title', ''),
                    },
                    'status': item.get('status', 'unknown')
                })
            
            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏
            if i + BATCH_SIZE < len(urls):
                time.sleep(2)
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –±–∞—Ç—á–∞: {e}")
            # –î–æ–±–∞–≤–∏—Ç—å –ø—É—Å—Ç—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –æ—à–∏–±–∫–æ–π
            for url in batch:
                results.append({
                    'url': url,
                    'html_content': None,
                    'metadata': {'status': 'error', 'error': str(e)},
                    'status': 'error'
                })
    
    return results


def save_to_staging(conn, results: List[Dict]):
    """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ staging —Ç–∞–±–ª–∏—Ü—É"""
    cursor = conn.cursor()
    
    success_count = 0
    error_count = 0
    skipped_count = 0
    
    for result in results:
        url = result['url']
        cqc_id = extract_cqc_id_from_url(url)
        html_content = result.get('html_content')
        metadata = result.get('metadata', {})
        status = result.get('status', 'unknown')
        
        if not html_content:
            print(f"‚ö†Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–æ (–Ω–µ—Ç HTML): {url}")
            error_count += 1
            continue
        
        try:
            cursor.execute("""
                INSERT INTO autumna_staging (
                    source_url,
                    cqc_location_id,
                    scraped_at,
                    html_content,
                    firecrawl_metadata
                ) VALUES (
                    %(url)s,
                    %(cqc_id)s,
                    CURRENT_TIMESTAMP,
                    %(html_content)s,
                    %(metadata)s::jsonb
                )
                ON CONFLICT (source_url) DO UPDATE
                SET 
                    html_content = EXCLUDED.html_content,
                    firecrawl_metadata = EXCLUDED.firecrawl_metadata,
                    scraped_at = EXCLUDED.scraped_at,
                    updated_at = CURRENT_TIMESTAMP
                RETURNING id
            """, {
                'url': url,
                'cqc_id': cqc_id,
                'html_content': html_content,
                'metadata': json.dumps(metadata)
            })
            
            staging_id = cursor.fetchone()[0]
            success_count += 1
            print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {url} (ID: {staging_id})")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ {url}: {e}")
            error_count += 1
    
    conn.commit()
    cursor.close()
    
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   ‚úÖ –£—Å–ø–µ—à–Ω–æ: {success_count}")
    print(f"   ‚ùå –û—à–∏–±–∫–∏: {error_count}")
    print(f"   ‚è≠Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–æ: {skipped_count}")
    
    return success_count, error_count


def load_urls_from_file(filepath: str) -> List[str]:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å —Å–ø–∏—Å–æ–∫ URL –∏–∑ —Ñ–∞–π–ª–∞"""
    urls = []
    with open(filepath, 'r') as f:
        for line in f:
            url = line.strip()
            if url and url.startswith('http'):
                urls.append(url)
    return urls


def main():
    parser = argparse.ArgumentParser(description='–§–∞–∑–∞ 1: –ó–∞–≥—Ä—É–∑–∫–∞ HTML –∏–∑ Firecrawl –≤ staging')
    parser.add_argument('--urls', required=True, help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å–æ —Å–ø–∏—Å–∫–æ–º URL')
    parser.add_argument('--api-key', required=True, help='Firecrawl API –∫–ª—é—á')
    parser.add_argument('--dry-run', action='store_true', help='–¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—É—Å–∫ –±–µ–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è')
    
    args = parser.parse_args()
    
    # –ó–∞–≥—Ä—É–∑–∏—Ç—å URLs
    print(f"üìã –ó–∞–≥—Ä—É–∑–∫–∞ URLs –∏–∑ {args.urls}...")
    urls = load_urls_from_file(args.urls)
    print(f"   –ù–∞–π–¥–µ–Ω–æ {len(urls)} URLs")
    
    if args.dry_run:
        print("üß™ DRY RUN - URLs –Ω–µ –±—É–¥—É—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω—ã –≤ Firecrawl")
        print("\n–ü–µ—Ä–≤—ã–µ 5 URLs:")
        for url in urls[:5]:
            print(f"  - {url}")
        return
    
    # –ü–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ –ë–î
    print("\nüîå –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î...")
    conn = get_db_connection()
    print("   ‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–æ")
    
    # –°–∫—Ä–∞–ø–∏—Ç—å —á–µ—Ä–µ–∑ Firecrawl
    print("\nüöÄ –ó–∞–ø—É—Å–∫ Firecrawl —Å–∫—Ä–∞–ø–∏–Ω–≥–∞...")
    results = scrape_urls_with_firecrawl(urls, args.api_key)
    
    # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ staging
    print("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ staging —Ç–∞–±–ª–∏—Ü—É...")
    success, errors = save_to_staging(conn, results)
    
    conn.close()
    
    print(f"\n‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ! –£—Å–ø–µ—à–Ω–æ: {success}, –û—à–∏–±–æ–∫: {errors}")


if __name__ == '__main__':
    main()

